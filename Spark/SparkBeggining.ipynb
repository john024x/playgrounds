{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MPw1uJuykPZ8DNM-x1gYzQJp2qVLRKE0","timestamp":1672373418275}],"authorship_tag":"ABX9TyMc1km4SYsL+Fuy0e6JC/95"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Zf4by4vnEBPW","executionInfo":{"status":"ok","timestamp":1672373457204,"user_tz":360,"elapsed":3,"user":{"displayName":"Juan Fuentes","userId":"12340410414890777706"}}},"outputs":[],"source":["#!apt-get install openjdk-8-jdk-headless -qq > /dev/null"]},{"cell_type":"code","source":["!pip install pyspark py4j"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZm7g180IT4X","executionInfo":{"status":"ok","timestamp":1672373509556,"user_tz":360,"elapsed":52355,"user":{"displayName":"Juan Fuentes","userId":"12340410414890777706"}},"outputId":"741f40f3-98d8-4789-f2c9-f3e5e2ffbf47"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 43 kB/s \n","\u001b[?25hCollecting py4j\n","  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n","\u001b[K     |████████████████████████████████| 200 kB 59.2 MB/s \n","\u001b[?25h  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[K     |████████████████████████████████| 199 kB 12.8 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=6a4a484763f508a21418335856b165619ff1bbb0a855cc4c7370f4e9573c3509\n","  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"]}]},{"cell_type":"code","source":["from pyspark import SparkContext"],"metadata":{"id":"Cev71FdDMU-L","executionInfo":{"status":"ok","timestamp":1672373509557,"user_tz":360,"elapsed":5,"user":{"displayName":"Juan Fuentes","userId":"12340410414890777706"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"id":"-tIQR6vlMZ8D","executionInfo":{"status":"ok","timestamp":1672373509557,"user_tz":360,"elapsed":4,"user":{"displayName":"Juan Fuentes","userId":"12340410414890777706"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["spark = SparkSession.builder.master(\"local\").appName(\"Sparksito\").getOrCreate()"],"metadata":{"id":"NVICcfwSM1c_","executionInfo":{"status":"ok","timestamp":1672373518713,"user_tz":360,"elapsed":9160,"user":{"displayName":"Juan Fuentes","userId":"12340410414890777706"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"j8umwHQENOfY","executionInfo":{"status":"ok","timestamp":1672373518908,"user_tz":360,"elapsed":201,"user":{"displayName":"Juan Fuentes","userId":"12340410414890777706"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["sc = SparkContext(master=\"local\",appName=\"Contexto\")"],"metadata":{"id":"464JclLyOD4A","executionInfo":{"status":"ok","timestamp":1672373519483,"user_tz":360,"elapsed":577,"user":{"displayName":"Juan Fuentes","userId":"12340410414890777706"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["spark2 = SparkSession(sc)\n","spark2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"5PVQrchtOIPR","executionInfo":{"status":"ok","timestamp":1672373521389,"user_tz":360,"elapsed":1908,"user":{"displayName":"Juan Fuentes","userId":"12340410414890777706"}},"outputId":"7e82744c-bf2c-4665-e25b-daa344cb7965"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f3a43f9d850>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://8e00c895d78e:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Contexto</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["###RDD y DataFrames\n","Todas las aplicaciones en Spark poseen un manejador central de programa (Driver) y varios ejecutores que se crean a lo largo del clúster, estas son las computadoras que realizarán las tareas en paralelo y finalmente devolverán los valores al driver, la aplicación central.\n","\n","\n","##RDD\n","Para poder realizar estas tareas, Spark posee desde su versión 1.0 los RDD (Resilient Distributed Dataset), los cuales son tolerantes a fallos y pueden ser distribuidos a lo largo de los nodos del clúster.\n","\n","Los RDD pueden ser creados al cargar datos de manera distribuida, como es desde un HDFS, Cassanda, Hbase o cualquier sistema de datos soportado por Hadoop, pero también por colecciones de datos de Scala o Python, además de poder ser leídos desde archivos en el sistema local.\n","\n","En visión general, un RDD puede ser visto como un set de datos los cuales soportan solo dos tipos de operaciones: transformaciones y acciones.\n","\n","Las transformaciones permiten crear un nuevo RDD a partir de uno previamente existente, mientras que las acciones retornan un valor al driver de la aplicación. El núcleo de operación del paradigma de Spark es la ejecución perezosa (Lazy), es decir que las transformaciones solo serán calculadas posterior a una llamada de acción.\n","\n","Además, los RDD poseen una familiaridad con el paradigma orientado a objetos, lo cual permite que podamos realizar operaciones de bajo nivel a modo. Map, filter y reduce son tres de las operaciones más comunes.\n","\n","Una de las grandes ventajas que ofrecen los RDD es la compilación segura; por su particularidad de ejecución perezosa, se calcula si se generará un error o no antes de ejecutarse, lo cual permite identificar problemas antes de lanzar la aplicación. El pero que podemos encontrar con los RDD es que no son correctamente tratados por el Garbage collector y cuando las lógicas de operación se hacen complejas, su uso puede resultar poco práctico, aquí entran los DataFrames.\n","\n","##DataFrames\n","Esos componentes fueron agregados en la versión 1.3 de Spark y pueden ser invocados con el contexto espacial de Spark SQL. Como indica su nombre, es un módulo especialmente desarrollado para ser ejecutado con instrucciones parecidas al SQL estándar.\n","\n","De la misma forma, como los RDD, estos pueden ser creados a partir de archivos, tablas tipo Hive, bases de datos externas y RDD o DataFrames existentes.\n","\n","El primer detalle que salta cuando creamos un DataFrame es que poseen columnas nombradas, lo que a nivel conceptual es como trabajar con un DataFrame de Pandas. Con la excepción que a nivel interno Spark trabaja con Scala, lo cual le asigna a cada columna el tipo de dato Row, un tipo especial de objeto sin tipo definido.\n","\n","Pero no es todo, los DataFrames implementan un sistema llamado Catalyst, el cual es un motor de optimización de planes de ejecución, parecido al que usan las bases de datos, pero diseñado para la cantidad de datos propia de Spark, aunado a eso, se tiene implementado un optimizador de memoria y consumo de CPU llamado Tungsten, el cual determina cómo se convertirán los planes lógicos creados por Catalyst a un plan físico."],"metadata":{"id":"EXECt66YPP3L"}}]}